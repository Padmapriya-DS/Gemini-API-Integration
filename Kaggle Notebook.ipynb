{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f272c79b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T08:15:48.309761Z",
     "iopub.status.busy": "2025-11-10T08:15:48.309455Z",
     "iopub.status.idle": "2025-11-10T08:15:48.380937Z",
     "shell.execute_reply": "2025-11-10T08:15:48.380055Z"
    },
    "papermill": {
     "duration": 0.077481,
     "end_time": "2025-11-10T08:15:48.382588",
     "exception": false,
     "start_time": "2025-11-10T08:15:48.305107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099cd609",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T08:15:48.389965Z",
     "iopub.status.busy": "2025-11-10T08:15:48.389015Z",
     "iopub.status.idle": "2025-11-10T08:15:53.135876Z",
     "shell.execute_reply": "2025-11-10T08:15:53.134995Z"
    },
    "papermill": {
     "duration": 4.752163,
     "end_time": "2025-11-10T08:15:53.137440",
     "exception": false,
     "start_time": "2025-11-10T08:15:48.385277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac8f6c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T08:15:53.144059Z",
     "iopub.status.busy": "2025-11-10T08:15:53.143360Z",
     "iopub.status.idle": "2025-11-10T08:15:53.575253Z",
     "shell.execute_reply": "2025-11-10T08:15:53.574205Z"
    },
    "papermill": {
     "duration": 0.436963,
     "end_time": "2025-11-10T08:15:53.576861",
     "exception": false,
     "start_time": "2025-11-10T08:15:53.139898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-05-20\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-flash-lite-preview-06-17\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-preview-image-generation\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-preview\n",
      "models/veo-3.0-fast-generate-preview\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.0-flash-live-001\n",
      "models/gemini-live-2.5-flash-preview\n",
      "models/gemini-2.5-flash-live-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "for m in genai.list_models():\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c84c799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T08:15:53.583953Z",
     "iopub.status.busy": "2025-11-10T08:15:53.583194Z",
     "iopub.status.idle": "2025-11-10T08:15:54.836560Z",
     "shell.execute_reply": "2025-11-10T08:15:54.835678Z"
    },
    "papermill": {
     "duration": 1.258337,
     "end_time": "2025-11-10T08:15:54.837966",
     "exception": false,
     "start_time": "2025-11-10T08:15:53.579629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-flash-latest\")\n",
    "\n",
    "response = model.generate_content(\"Hello! Reply with a short greeting only.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b695b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T08:15:54.844235Z",
     "iopub.status.busy": "2025-11-10T08:15:54.843936Z",
     "iopub.status.idle": "2025-11-10T08:15:59.856616Z",
     "shell.execute_reply": "2025-11-10T08:15:59.855695Z"
    },
    "papermill": {
     "duration": 5.017556,
     "end_time": "2025-11-10T08:15:59.858072",
     "exception": false,
     "start_time": "2025-11-10T08:15:54.840516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 3 ideas for YouTube Shorts related to AI, designed for quick engagement and virality:\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Idea: **\"AI Image Generator's WORST Fails\"**\n",
      "\n",
      "* **Concept:** Showcase hilarious and unexpected failures (glitches, distortions, extra fingers, weird anatomy) that occur when prompting popular AI image generators (like Midjourney, DALL-E 3, or Stable Diffusion).\n",
      "* **Format:** A rapid-fire compilation clip (15-30 seconds).\n",
      "* **Execution/Hook:**\n",
      "    1. **Text Overlay (Top):** \"Prompt: 'A majestic dragon riding a bicycle through the city.'\"\n",
      "    2. **Image 1 (0-3 seconds):** Show the weirdly distorted AI result (the dragon has 8 legs, the bicycle is melted).\n",
      "    3. **Rapid Transition (0.5 seconds):** Switch to the next fail immediately.\n",
      "    4. **Text Overlay (Top):** \"Prompt: 'A friendly dog holding a cup of coffee.'\"\n",
      "    5. **Image 2 (4-7 seconds):** Show the horrifying dog result (the coffee cup is fused into its hand, it has three eyes).\n",
      "* **Why it works:** Comedy, relatability (everyone who uses these tools experiences fails), and the content is instantly visually striking.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Idea: **\"The Instant AI Music Video Challenge\"**\n",
      "\n",
      "* **Concept:** Demonstrate how quickly AI tools can create an entire song and corresponding visuals, challenging the traditional creative pipeline.\n",
      "* **Format:** A time-lapse demonstration and final reveal (45-55 seconds).\n",
      "* **Execution/Hook:**\n",
      "    1. **Start (0-5 seconds):** \"Can AI create a full Music Video in 5 minutes? Let's find out.\" (High energy, catchy music starts).\n",
      "    2. **Segment 1 (6-20 seconds):** Use an AI music generator (like Suno or Udio) to create a song based on a single prompt (\"80s synthwave song about pizza\"). Show the text input and the music generation progress bar.\n",
      "    3. **Segment 2 (21-35 seconds):** Switch to an AI video/image generator (like Pika, Runway, or Midjourney) and rapidly create 4-5 clips or images to match the song's theme.\n",
      "    4. **Final Reveal (36-55 seconds):** Play the final 15-20 second snippet of the complete, AI-generated song and video.\n",
      "* **Why it works:** Shows the impressive speed of AI and highlights specific tools, making it educational and jaw-dropping.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Idea: **\"AI Chatbot Role-Play: Your New Therapist?\"**\n",
      "\n",
      "* **Concept:** Set up a humorous or surprising scenario using a sophisticated AI chatbot (like ChatGPT 4o or Claude 3) where the AI takes on a dramatic or unusual role.\n",
      "* **Format:** Screen recording of the chat combined with text-to-speech or voiceover (30-40 seconds).\n",
      "* **Execution/Hook:**\n",
      "    1. **Visual:** Screen recording of the chat interface.\n",
      "    2. **Voiceover/Text-to-Speech:**\n",
      "        * **User Prompt:** \"Act as a dramatic, overly poetic Shakespearean villain who is trying to apologize for stealing my socks.\"\n",
      "        * **AI Response (Read dramatically):** \"Hark! This heart, a wretched, mildewed thing, doth weep for the peasant linens I usurped! Forgive this thief of thread, lest my soul is banished to the cold, lonely washing machine of regret!\"\n",
      "    3. **Ending Call-to-Action:** \"What impossible role should I give it next? üëá\"\n",
      "* **Why it works:** Highlights the AI's creativity, focuses on prompt engineering, and is inherently shareable due to the absurdity of the dialogue.\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"Give me 3 ideas for YouTube shorts related to AI.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "117cf4ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T08:15:59.864999Z",
     "iopub.status.busy": "2025-11-10T08:15:59.864332Z",
     "iopub.status.idle": "2025-11-10T08:16:04.307881Z",
     "shell.execute_reply": "2025-11-10T08:16:04.306740Z"
    },
    "papermill": {
     "duration": 4.448607,
     "end_time": "2025-11-10T08:16:04.309471",
     "exception": false,
     "start_time": "2025-11-10T08:15:59.860864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a great topic! Python decorators are powerful, but they often sound more complicated than they actually are.\n",
      "\n",
      "Here is an explanation of Python decorators broken down into simple words, an analogy, and the essential technical details.\n",
      "\n",
      "---\n",
      "\n",
      "## üêç Python Decorators: The Simple Explanation\n",
      "\n",
      "Imagine you have a function that already does a job (like baking a cake). A **decorator** is just a simple way to **wrap** or **modify** that existing function to add new features or change its behavior, **without actually changing the function's original code.**\n",
      "\n",
      "### üç∞ The Analogy (Adding Icing to a Cake)\n",
      "\n",
      "1.  **The Original Function (The Cake):** You have a function called `bake_cake()`. It works perfectly fine and produces a plain vanilla cake.\n",
      "2.  **The Decorator (The Icing/Sprinkles):** You want the cake to be special. You create a decorator called `@add_icing`.\n",
      "3.  **The Application:** Instead of opening up the `bake_cake()` recipe and adding \"Step 10: Put on Icing,\" you simply put the decorator right before your function definition:\n",
      "\n",
      "```python\n",
      "@add_icing # <--- This is the decorator\n",
      "def bake_cake():\n",
      "    print(\"Cake is baked.\")\n",
      "    # (The original code remains untouched)\n",
      "```\n",
      "\n",
      "**The Result:** Now, whenever you call `bake_cake()`, it automatically executes the icing logic *around* the cake baking, making it tastier, but the core `bake_cake` process is exactly the same.\n",
      "\n",
      "---\n",
      "\n",
      "## üõ†Ô∏è The Essential Technical Details\n",
      "\n",
      "Technically speaking, a decorator is just a function that **takes another function as an input and returns a modified version of that function.**\n",
      "\n",
      "### 1. Functions are First-Class Objects\n",
      "\n",
      "The foundation of decorators is the idea that in Python, **functions can be treated like variables** (or \"first-class objects\"). This means you can:\n",
      "*   Pass a function to another function as an argument.\n",
      "*   Return a function from another function.\n",
      "*   Store a function in a variable.\n",
      "\n",
      "### 2. The `@` Syntax (Syntactic Sugar)\n",
      "\n",
      "The `@decorator_name` syntax is just Python's shortcut (or \"syntactic sugar\") for doing this manually:\n",
      "\n",
      "**The Shortcut (Using `@`):**\n",
      "\n",
      "```python\n",
      "@my_decorator\n",
      "def say_hello():\n",
      "    print(\"Hello!\")\n",
      "```\n",
      "\n",
      "**What Python is actually doing (Manual Wrapping):**\n",
      "\n",
      "```python\n",
      "def say_hello():\n",
      "    print(\"Hello!\")\n",
      "\n",
      "# The function is passed to the decorator,\n",
      "# and the result (the wrapped function) replaces the original\n",
      "say_hello = my_decorator(say_hello)\n",
      "```\n",
      "\n",
      "### 3. Common Uses\n",
      "\n",
      "Decorators are everywhere in Python because they promote clean, reusable code.\n",
      "\n",
      "| Use Case | Description | Example |\n",
      "| :--- | :--- | :--- |\n",
      "| **Logging** | Automatically log when a function starts and finishes. | `@log_execution` |\n",
      "| **Timing** | Measure how long a function takes to run. | `@time_it` |\n",
      "| **Permission** | Check if a user is logged in or has the right permissions before running a sensitive function (common in web frameworks like Django/Flask). | `@login_required` |\n",
      "| **Caching** | Store the results of an expensive function call so it doesn't have to run again. | `@functools.cache` |\n",
      "\n",
      "---\n",
      "\n",
      "## Summary\n",
      "\n",
      "> A Python decorator is a **design pattern** that lets you easily and cleanly **add features** to an existing function (or class) by **wrapping** it inside another function, typically using the simple `@` syntax.\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content([\n",
    "    \"You are a coding assistant.\",\n",
    "    \"Explain Python decorators in simple words.\"\n",
    "])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc8dbbb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T08:16:04.316580Z",
     "iopub.status.busy": "2025-11-10T08:16:04.316308Z",
     "iopub.status.idle": "2025-11-10T08:16:10.484838Z",
     "shell.execute_reply": "2025-11-10T08:16:10.483630Z"
    },
    "papermill": {
     "duration": 6.173947,
     "end_time": "2025-11-10T08:16:10.486418",
     "exception": false,
     "start_time": "2025-11-10T08:16:04.312471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `yield` keyword in Python is one of the most powerful and often misunderstood features of the language.\n",
      "\n",
      "As an expert Python teacher, I'm going to break down exactly what `yield` does, why it's used, and how it differs from the standard `return` statement, all with a simple, illustrative example.\n",
      "\n",
      "---\n",
      "\n",
      "## The Core Concept: `yield` Makes a Function a Generator\n",
      "\n",
      "When a Python function contains the `yield` keyword, it automatically stops being a regular function and becomes a **generator function**.\n",
      "\n",
      "A generator function doesn't return a single value and exit; instead, it returns an **iterator** (the generator object) that can be paused and resumed on demand.\n",
      "\n",
      "### 1. The Standard `return`\n",
      "\n",
      "The `return` statement gives a value back to the caller, and the function's state is **destroyed** (it starts from the beginning next time).\n",
      "\n",
      "### 2. The `yield` Statement\n",
      "\n",
      "The `yield` statement gives a value back to the caller, but the function's state is **frozen** (paused). When the caller asks for the next value, the function resumes *exactly* where it left off, including all local variables and execution points.\n",
      "\n",
      "---\n",
      "\n",
      "## Simple Example: Counting with `yield`\n",
      "\n",
      "Let's compare a standard list-generating function with a generator function. We want a function that generates the numbers 1, 2, and 3.\n",
      "\n",
      "### Scenario A: Using Standard `return` (List Creation)\n",
      "\n",
      "```python\n",
      "def regular_counter():\n",
      "    \"\"\"\n",
      "    This function computes all values and stores them \n",
      "    in memory (a list) before returning.\n",
      "    \"\"\"\n",
      "    results = []\n",
      "    print(\"Starting counter.\")\n",
      "    \n",
      "    results.append(1)\n",
      "    results.append(2)\n",
      "    results.append(3)\n",
      "    \n",
      "    print(\"Finished computing all numbers.\")\n",
      "    return results\n",
      "\n",
      "# CALLING THE FUNCTION\n",
      "all_numbers = regular_counter()\n",
      "# Notice that the entire function runs immediately when called.\n",
      "\n",
      "print(\"\\n--- Iterating through results ---\")\n",
      "for num in all_numbers:\n",
      "    print(f\"Received: {num}\")\n",
      "```\n",
      "\n",
      "**Output Analysis (Scenario A):**\n",
      "1. `regular_counter()` runs entirely, printing \"Starting\" and \"Finished.\"\n",
      "2. The entire list `[1, 2, 3]` is built in memory.\n",
      "3. We then iterate over the *already existing* list.\n",
      "\n",
      "### Scenario B: Using `yield` (Generator Creation)\n",
      "\n",
      "```python\n",
      "def generator_counter():\n",
      "    \"\"\"\n",
      "    This generator function yields values one by one, \n",
      "    pausing and resuming execution.\n",
      "    \"\"\"\n",
      "    print(\"--- Starting generator ---\")\n",
      "    \n",
      "    # 1. Yields 1, then PAUSES here.\n",
      "    yield 1 \n",
      "    print(\"--- Resumed after 1 ---\")\n",
      "    \n",
      "    # 2. Yields 2, then PAUSES here.\n",
      "    yield 2\n",
      "    print(\"--- Resumed after 2 ---\")\n",
      "    \n",
      "    # 3. Yields 3, then PAUSES here.\n",
      "    yield 3\n",
      "    print(\"--- Generator finished ---\")\n",
      "    # After the last yield, the function raises StopIteration implicitly.\n",
      "\n",
      "# CALLING THE FUNCTION\n",
      "# This only creates the generator object; the function code doesn't run yet!\n",
      "my_generator = generator_counter()\n",
      "print(\"Generator object created, but function body has not executed.\")\n",
      "\n",
      "print(\"\\n--- Iterating through results ---\")\n",
      "for num in my_generator:\n",
      "    print(f\"Received: {num}\")\n",
      "```\n",
      "\n",
      "**Output Analysis (Scenario B):**\n",
      "1. When `generator_counter()` is called, we only see \"Generator object created.\" The code inside the generator is dormant.\n",
      "2. The `for` loop starts, acting as the consumer.\n",
      "3. The generator runs up to `yield 1`, prints \"Starting generator,\" returns 1, and **PAUSES**.\n",
      "4. The consumer prints \"Received: 1.\"\n",
      "5. The `for` loop asks for the next item.\n",
      "6. The generator **RESUMES** (printing \"Resumed after 1\"), runs up to `yield 2`, returns 2, and **PAUSES** again.\n",
      "7. The consumer prints \"Received: 2.\"\n",
      "8. This process continues until the generator finishes its last statement (printing \"Generator finished\"), and the loop terminates.\n",
      "\n",
      "---\n",
      "\n",
      "## Why Use `yield`? The Benefits of Generators\n",
      "\n",
      "The primary reasons generators are preferred in professional Python code are related to **efficiency** and **memory management**:\n",
      "\n",
      "### 1. Lazy Evaluation (On-Demand)\n",
      "\n",
      "Generators only compute the next item when requested. This is known as **lazy evaluation**. If you only need the first 10 items of a sequence of a million, a generator only computes those 10. A standard function computes all million items upfront.\n",
      "\n",
      "### 2. Memory Efficiency (Scalability)\n",
      "\n",
      "The generator never holds the entire sequence in memory. It holds only the state needed to calculate the *next* item. This is critical when dealing with very large datasets (like reading a huge log file line by line) where loading everything into memory simultaneously would crash your system.\n",
      "\n",
      "### 3. Infinite Sequences\n",
      "\n",
      "You can create a generator for an infinite sequence (like all prime numbers) because you never have to worry about running out of memory. The consumer just keeps asking for the next item as long as they need it.\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"You are an expert Python teacher.\n",
    "    Explain yield in Python with a simple example.\"\"\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2115004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T08:16:10.493876Z",
     "iopub.status.busy": "2025-11-10T08:16:10.493507Z",
     "iopub.status.idle": "2025-11-10T08:16:13.933421Z",
     "shell.execute_reply": "2025-11-10T08:16:13.932485Z"
    },
    "papermill": {
     "duration": 3.445576,
     "end_time": "2025-11-10T08:16:13.935076",
     "exception": false,
     "start_time": "2025-11-10T08:16:10.489500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am a large language model, trained by Google.\n",
      "\n",
      "I can do a wide variety of things, including:\n",
      "\n",
      "* **Answer questions:** About virtually any topic, using my extensive knowledge base.\n",
      "* **Generate creative text formats:** Like poems, code, scripts, musical pieces, email, letters, etc.\n",
      "* **Summarize information:** Taking long texts or articles and extracting the key points.\n",
      "* **Translate languages:** Between many different languages.\n",
      "* **Write and debug code:** In various programming languages.\n",
      "* **Offer explanations and tutorials:** Breaking down complex concepts.\n",
      "* **Engage in natural conversation:** Just like we are now!\n",
      "\n",
      "**How can I help you or what would you like to do first?**\n",
      "**Big Data** describes massive, diverse, and rapidly growing datasets that require specialized technologies to analyze and extract valuable insights, often characterized by Volume, Velocity, and Variety.\n"
     ]
    }
   ],
   "source": [
    "chat = model.start_chat()\n",
    "\n",
    "print(chat.send_message(\"Hello, what can you do?\").text)\n",
    "print(chat.send_message(\"Explain Big Data in one sentence.\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa94584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T08:16:13.942930Z",
     "iopub.status.busy": "2025-11-10T08:16:13.942594Z",
     "iopub.status.idle": "2025-11-10T08:16:15.896626Z",
     "shell.execute_reply": "2025-11-10T08:16:15.895393Z"
    },
    "papermill": {
     "duration": 1.959839,
     "end_time": "2025-11-10T08:16:15.898160",
     "exception": false,
     "start_time": "2025-11-10T08:16:13.938321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import pandas as pd\n",
      "import io\n",
      "\n",
      "# --- 1. Simulate loading a CSV file ---\n",
      "# In a real-world scenario, you would use:\n",
      "# df = pd.read_csv('your_file_name.csv')\n",
      "\n",
      "# For this example to be runnable, we create a dummy CSV content\n",
      "csv_data = \"\"\"Name,Age,City,Salary\n",
      "Alice,30,New York,70000\n",
      "Bob,25,Los Angeles,60000\n",
      "Charlie,35,Chicago,85000\n",
      "David,28,Houston,65000\n",
      "Eve,40,Miami,90000\n",
      "Frank,22,Boston,55000\n",
      "\"\"\"\n",
      "\n",
      "# Load the dummy CSV data into a pandas DataFrame\n",
      "df = pd.read_csv(io.StringIO(csv_data))\n",
      "\n",
      "\n",
      "# --- 2. Show the first 5 rows ---\n",
      "print(\"--- First 5 Rows of the DataFrame ---\")\n",
      "print(df.head(5))\n",
      "print(\"\\n\")\n",
      "\n",
      "\n",
      "# --- 3. Print the column names ---\n",
      "print(\"--- Column Names ---\")\n",
      "# Convert the Index object of columns into a list for clean printing\n",
      "column_names = df.columns.tolist()\n",
      "for col in column_names:\n",
      "    print(f\"- {col}\")\n",
      "\n",
      "# Or simply print the underlying Index object\n",
      "# print(df.columns)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"Write Python code to load a CSV file, show first 5 rows, and print column names.\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00bf499b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T08:16:15.906488Z",
     "iopub.status.busy": "2025-11-10T08:16:15.906196Z",
     "iopub.status.idle": "2025-11-10T08:16:20.016322Z",
     "shell.execute_reply": "2025-11-10T08:16:20.015030Z"
    },
    "papermill": {
     "duration": 4.116349,
     "end_time": "2025-11-10T08:16:20.018206",
     "exception": false,
     "start_time": "2025-11-10T08:16:15.901857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided DataFrame:\n",
      "\n",
      "| Name | Score |\n",
      "|------|-------|\n",
      "| A    | 45    |\n",
      "| B    | 78    |\n",
      "| C    | 90    |\n",
      "\n",
      "Here are 3 insights:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Significant Range in Performance\n",
      "The scores exhibit a wide range, spanning from a low of 45 (achieved by 'A') to a high of 90 (achieved by 'C'). This suggests a **substantial difference in performance** or proficiency among the individuals recorded.\n",
      "\n",
      "* **Supporting Metric:** The range is $90 - 45 = 45$.\n",
      "\n",
      "### 2. Strong Overall Central Tendency\n",
      "Despite the lowest score being 45, the average (mean) score is relatively high (71). This indicates that the group, as a whole, performed moderately well, and the scores are **leaning toward the higher end of the scale** (especially considering the maximum is 90).\n",
      "\n",
      "* **Supporting Metric:** The Mean Score is $(45 + 78 + 90) / 3 = 71$.\n",
      "\n",
      "### 3. Clear High Performer\n",
      "'C' is the clear high-performer, achieving the maximum possible score of 90 (assuming 100 is the ceiling, or simply the highest recorded score). This individual is performing significantly better than the group average (71) and the low performer (A, 45).\n",
      "\n",
      "* **Supporting Metric:** Score C (90) is 12 points higher than the next closest score (B, 78).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": [\"A\", \"B\", \"C\"],\n",
    "    \"Score\": [45, 78, 90]\n",
    "})\n",
    "\n",
    "response = model.generate_content(\n",
    "    f\"Analyze this DataFrame and give 3 insights:\\n{df.to_string()}\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7518e",
   "metadata": {
    "papermill": {
     "duration": 0.003118,
     "end_time": "2025-11-10T08:16:20.024590",
     "exception": false,
     "start_time": "2025-11-10T08:16:20.021472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this notebook, I demonstrated how to connect Gemini API in Kaggle, run basic text generation, chat, summarization, code generation and data analysis. You can now extend this to work with your own datasets or build AI-powered data projects."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 39.237368,
   "end_time": "2025-11-10T08:16:22.802416",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-10T08:15:43.565048",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
